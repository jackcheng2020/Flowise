{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackcheng2020/Flowise/blob/main/jack_cheng_gpt4all_langchain_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a005358",
      "metadata": {
        "id": "5a005358"
      },
      "source": [
        "# GPT4All Langchain Demo\n",
        "Originally made by [ouseful]('https://gist.github.com/psychemedia/51f45fbfe160f78605bdd0c1b404e499') but edited by segestic .*\n",
        "Example of locally running [`GPT4All`](https://github.com/nomic-ai/gpt4all), a 4GB, *llama.cpp* based large language model (LLM) under [`langchain`](https://github.com/hwchase17/langchain), in a Jupyter notebook running a Python 3.9 kernel.\n",
        "\n",
        "*Tested on Google Colab using CPU .*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d226d19c",
      "metadata": {
        "id": "d226d19c"
      },
      "source": [
        "## Model preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dde35bcc",
      "metadata": {
        "id": "dde35bcc"
      },
      "source": [
        "- download `gpt4all` model:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd /content/gdrive/MyDrive/Gpt4allfiles"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSvIW9xZgjth",
        "outputId": "abaca7e0-d882-4813-d530-ba146efee42c"
      },
      "id": "MSvIW9xZgjth",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Gpt4allfiles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Gpt4allfiles\n",
        "\n",
        "%ls\n",
        "\n",
        "!free -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWKcPk5Dgwfm",
        "outputId": "ecf9d621-2c8d-4fac-8f65-72ecedd69ca9"
      },
      "id": "FWKcPk5Dgwfm",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Gpt4allfiles\n",
            "gpt4all-lora-quantized.bin  gpt4all-lora-quantized.bin.1  \u001b[0m\u001b[01;34mllama\u001b[0m/\n",
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            12Gi       807Mi       9.3Gi       1.0Mi       2.6Gi        11Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d5237686",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5237686",
        "outputId": "810f50ee-2406-4fe1-da8c-b0ebbfdf57c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-06 01:01:41--  https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized.bin\n",
            "Resolving the-eye.eu (the-eye.eu)... 162.213.130.6\n",
            "Connecting to the-eye.eu (the-eye.eu)|162.213.130.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4212732137 (3.9G) [application/octet-stream]\n",
            "Saving to: ‘gpt4all-lora-quantized.bin.2’\n",
            "\n",
            "gpt4all-lora-quanti 100%[===================>]   3.92G  7.54MB/s    in 8m 52s  \n",
            "\n",
            "2024-02-06 01:10:34 (7.55 MB/s) - ‘gpt4all-lora-quantized.bin.2’ saved [4212732137/4212732137]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized.bin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h09WLTviy9R",
        "outputId": "93819104-7a60-4872-d9dd-592f57e66c8a"
      },
      "id": "8h09WLTviy9R",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57520ec5",
      "metadata": {
        "id": "57520ec5"
      },
      "source": [
        "- download `llama.cpp` 7B model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37bed5fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37bed5fc",
        "outputId": "6e92908e-d2e8-431d-f397-bfdaa5965ec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyllama\n",
            "  Downloading pyllama-0.0.9-py3-none-any.whl (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pyllama) (2.1.0+cu121)\n",
            "Collecting fairscale>=0.4.13 (from pyllama)\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire~=0.5.0 (from pyllama)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hiq-python>=1.1.9 (from pyllama)\n",
            "  Downloading hiq_python-1.1.12-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.2/74.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.97 (from pyllama)\n",
            "  Downloading sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from fairscale>=0.4.13->pyllama) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire~=0.5.0->pyllama) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire~=0.5.0->pyllama) (2.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (6.0.1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (5.3.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (5.9.5)\n",
            "Collecting py-itree (from hiq-python>=1.1.9->pyllama)\n",
            "  Downloading py_itree-0.0.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.3/242.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (2.31.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (13.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pyllama) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->pyllama) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->pyllama) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->pyllama) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->hiq-python>=1.1.9->pyllama) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->hiq-python>=1.1.9->pyllama) (2.16.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pyllama) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->hiq-python>=1.1.9->pyllama) (0.1.2)\n",
            "Building wheels for collected packages: fairscale, fire\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332104 sha256=1533106f748931ae9c447455a7002f684bef9dc4697807ac8983b5db8183e59d\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=fc799a1e2b10dda9f7edf0a7de195575a5c116fe0f22b57d6e3d03fde30eb64b\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fairscale fire\n",
            "Installing collected packages: sentencepiece, py-itree, fire, hiq-python, fairscale, pyllama\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.99\n",
            "    Uninstalling sentencepiece-0.1.99:\n",
            "      Successfully uninstalled sentencepiece-0.1.99\n",
            "Successfully installed fairscale-0.4.13 fire-0.5.0 hiq-python-1.1.12 py-itree-0.0.19 pyllama-0.0.9 sentencepiece-0.1.97\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "❤️ Resume download is supported. You can ctrl-c and rerun the program to resume the downloading\n",
            "Downloading tokenizer...\n",
            "✅ llama//tokenizer.model\n",
            "✅ llama//tokenizer_checklist.chk\n",
            "tokenizer.model: OK\n",
            "Downloading 7B\n",
            "downloading file to llama//7B/consolidated.00.pth ...please wait for a few minutes ...\n"
          ]
        }
      ],
      "source": [
        "%pip install pyllama\n",
        "%pip install transformers\n",
        "!python3 -m llama.download --model_size 7B --folder llama/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c21e391",
      "metadata": {
        "id": "0c21e391"
      },
      "source": [
        "- transform `gpt4all` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c11c606",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c11c606",
        "outputId": "f39e1e03-8654-45fb-a9b9-32f5f6c25974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyllamacpp\n",
            "  Downloading pyllamacpp-1.0.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.4/269.4 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from pyllamacpp) (2.0.0+cu118)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from pyllamacpp) (0.1.97)\n",
            "Collecting streamlit-ace\n",
            "  Downloading streamlit_ace-0.1.1-py3-none-any.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit\n",
            "  Downloading streamlit-1.21.0-py2.py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<5,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (4.2.2)\n",
            "Collecting blinker>=1.0.0\n",
            "  Downloading blinker-1.6.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (8.4.0)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (6.2)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 KB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.4 in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (2.27.1)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (9.0.0)\n",
            "Requirement already satisfied: pandas<2,>=0.25 in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (2.8.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (0.10.2)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (13.3.3)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (4.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (8.1.3)\n",
            "Collecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (4.5.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (5.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (1.22.4)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (23.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (6.1.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.9/dist-packages (from streamlit->pyllamacpp) (3.20.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->pyllamacpp) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->pyllamacpp) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->pyllamacpp) (3.10.7)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->pyllamacpp) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->pyllamacpp) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->pyllamacpp) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->pyllamacpp) (16.0.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit->pyllamacpp) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit->pyllamacpp) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit->pyllamacpp) (0.4)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=1.4->streamlit->pyllamacpp) (3.15.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas<2,>=0.25->streamlit->pyllamacpp) (2022.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->pyllamacpp) (2.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil->streamlit->pyllamacpp) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit->pyllamacpp) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit->pyllamacpp) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit->pyllamacpp) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit->pyllamacpp) (1.26.15)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich>=10.11.0->streamlit->pyllamacpp) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich>=10.11.0->streamlit->pyllamacpp) (2.14.0)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal>=1.1->streamlit->pyllamacpp) (0.1.0.post0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from validators>=0.2->streamlit->pyllamacpp) (4.4.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->pyllamacpp) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit->pyllamacpp) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit->pyllamacpp) (22.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->streamlit->pyllamacpp) (0.1.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal>=1.1->streamlit->pyllamacpp) (2023.3)\n",
            "Building wheels for collected packages: validators\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=5a0f9957c70bf44b36195d327d26e834e52d061b66c4c69ad00fd23ac78988de\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/f0/a8/1094fca7a7e5d0d12ff56e0c64675d72aa5cc81a5fc200e849\n",
            "Successfully built validators\n",
            "Installing collected packages: watchdog, validators, smmap, pympler, blinker, pydeck, gitdb, gitpython, streamlit, streamlit-ace, pyllamacpp\n",
            "Successfully installed blinker-1.6.1 gitdb-4.0.10 gitpython-3.1.31 pydeck-0.8.0 pyllamacpp-1.0.6 pympler-1.0.1 smmap-5.0.0 streamlit-1.21.0 streamlit-ace-0.1.1 validators-0.20.0 watchdog-3.0.0\n",
            "Namespace(gpt4all_model='./gpt4all-lora-quantized.bin', tokenizer_model='llama/tokenizer.model', fout_path='./gpt4all-lora-q-converted.bin')\n",
            "converting ./gpt4all-lora-quantized.bin\n",
            "Processing part 1 of 1\n",
            "\n",
            "Processing tensor b'tok_embeddings.weight' with shape: [32001, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.0.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.0.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.0.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.0.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.0.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.0.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.0.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.0.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.0.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.1.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.1.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.1.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.1.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.1.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.1.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.1.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.1.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.1.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.2.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.2.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.2.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.2.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.2.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.2.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.2.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.2.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.2.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.3.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.3.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.3.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.3.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.3.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.3.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.3.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.3.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.3.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.4.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.4.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.4.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.4.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.4.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.4.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.4.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.4.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.4.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.5.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.5.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.5.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.5.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.5.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.5.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.5.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.5.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.5.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.6.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.6.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.6.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.6.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.6.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.6.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.6.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.6.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.6.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.7.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.7.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.7.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.7.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.7.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.7.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.7.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.7.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.7.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.8.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.8.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.8.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.8.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.8.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.8.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.8.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.8.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.8.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.9.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.9.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.9.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.9.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.9.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.9.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.9.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.9.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.9.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.10.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.10.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.10.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.10.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.10.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.10.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.10.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.10.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.10.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.11.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.11.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.11.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.11.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.11.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.11.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.11.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.11.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.11.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.12.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.12.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.12.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.12.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.12.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.12.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.12.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.12.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.12.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.13.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.13.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.13.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.13.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.13.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.13.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.13.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.13.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.13.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.14.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.14.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.14.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.14.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.14.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.14.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.14.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.14.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.14.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.15.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.15.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.15.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.15.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.15.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.15.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.15.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.15.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.15.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.16.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.16.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.16.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.16.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.16.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.16.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.16.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.16.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.16.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.17.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.17.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.17.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.17.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.17.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.17.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.17.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.17.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.17.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.18.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.18.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.18.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.18.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.18.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.18.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.18.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.18.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.18.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.19.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.19.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.19.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.19.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.19.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.19.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.19.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.19.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.19.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.20.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.20.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.20.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.20.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.20.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.20.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.20.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.20.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.20.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.21.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.21.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.21.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.21.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.21.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.21.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.21.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.21.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.21.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.22.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.22.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.22.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.22.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.22.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.22.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.22.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.22.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.22.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.23.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.23.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.23.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.23.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.23.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.23.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.23.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.23.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.23.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.24.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.24.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.24.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.24.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.24.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.24.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.24.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.24.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.24.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.25.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.25.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.25.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.25.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.25.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.25.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.25.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.25.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.25.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.26.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.26.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.26.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.26.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.26.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.26.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.26.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.26.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.26.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.27.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.27.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.27.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.27.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.27.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.27.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.27.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.27.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.27.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.28.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.28.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.28.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.28.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.28.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.28.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.28.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.28.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.28.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.29.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.29.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.29.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.29.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.29.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.29.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.29.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.29.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.29.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.30.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.30.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.30.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.30.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.30.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.30.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.30.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.30.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.30.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.31.attention.wq.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.31.attention.wk.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.31.attention.wv.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.31.attention.wo.weight' with shape: [4096, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.31.feed_forward.w1.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.31.feed_forward.w2.weight' with shape: [4096, 11008] and type: Q4_0\n",
            "Processing tensor b'layers.31.feed_forward.w3.weight' with shape: [11008, 4096] and type: Q4_0\n",
            "Processing tensor b'layers.31.attention_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'layers.31.ffn_norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'norm.weight' with shape: [4096] and type: F32\n",
            "Processing tensor b'output.weight' with shape: [32001, 4096] and type: Q4_0\n",
            "Done. Output file: ./gpt4all-lora-q-converted.bin\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%pip install pyllamacpp\n",
        "!pyllamacpp-convert-gpt4all ./gpt4all-lora-quantized.bin llama/tokenizer.model ./gpt4all-lora-q-converted.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a858d90f",
      "metadata": {
        "id": "a858d90f"
      },
      "outputs": [],
      "source": [
        "GPT4ALL_MODEL_PATH = \"./gpt4all-lora-q-converted.bin\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72d09e27",
      "metadata": {
        "id": "72d09e27"
      },
      "source": [
        "## `langchain` Demo\n",
        "\n",
        "Example of running a prompt using `langchain`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65c9ca4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65c9ca4d",
        "outputId": "ebbf5166-1dbd-4a03-825f-0a76cafd20d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping langchain as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/hwchase17/langchain.git\n",
            "  Cloning https://github.com/hwchase17/langchain.git to /tmp/pip-req-build-vgaftp6q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/hwchase17/langchain.git /tmp/pip-req-build-vgaftp6q\n",
            "  Resolved https://github.com/hwchase17/langchain.git to commit e63f9a846be7a85de7d3e3a1b277a4521b42808d\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.136) (1.10.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.136) (1.22.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.136) (6.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.136) (2.27.1)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openapi-schema-pydantic<2.0,>=1.2\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.136) (8.2.2)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.136) (1.4.47)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.136) (22.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.136) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<2,>=1->langchain==0.0.136) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.136) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.136) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.136) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.136) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.136) (23.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: langchain\n",
            "  Building wheel for langchain (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langchain: filename=langchain-0.0.136-py3-none-any.whl size=515694 sha256=68dc4422c85da9525a687efd61c6f2109d224e12b8d9441d6d7df4199e4e5b1f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t_kb3gvl/wheels/60/a8/47/1211cd32d951e2c51b318953d48c0a1f206f372a00cbe37cea\n",
            "Successfully built langchain\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 langchain-0.0.136 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.8.0 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "#https://python.langchain.com/en/latest/ecosystem/llamacpp.html\n",
        "%pip uninstall -y langchain\n",
        "%pip install --upgrade git+https://github.com/hwchase17/langchain.git\n",
        "\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain import PromptTemplate, LLMChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "500eb501",
      "metadata": {
        "id": "500eb501"
      },
      "source": [
        "- set up prompt template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60202ce2",
      "metadata": {
        "id": "60202ce2"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "Question: {question}\n",
        "Answer: Let's think step by step.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T1whQzdq9ut",
        "outputId": "0b7c371d-900b-43c8-cd3e-646bf5854e94"
      },
      "id": "2T1whQzdq9ut",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.1.29.tar.gz (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.9/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.29-cp39-cp39-linux_x86_64.whl size=138702 sha256=8f60a8bbeb625bc886e9e905c7971a86a514e6246560b8f01d5e2384e5379280\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/38/c8/54567a120a627f347110705e89820cb41a0698a93a3901ad41\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.1.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad12a227",
      "metadata": {
        "id": "ad12a227"
      },
      "source": [
        "- load model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d83c596",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d83c596",
        "outputId": "59e5a734-a3ef-4571-84a7-92db9e0a820b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 270 ms, sys: 1.36 s, total: 1.63 s\n",
            "Wall time: 36.7 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "llm = LlamaCpp(model_path=GPT4ALL_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a40b30f6",
      "metadata": {
        "id": "a40b30f6"
      },
      "source": [
        "- create language chain using prompt template and loaded model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112a5b4b",
      "metadata": {
        "id": "112a5b4b"
      },
      "outputs": [],
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5351ee46",
      "metadata": {
        "id": "5351ee46"
      },
      "source": [
        "- run prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41b6cbbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "41b6cbbd",
        "outputId": "0645a097-9b60-4fac-d4c4-58c8397ff588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4min 31s, sys: 386 ms, total: 4min 32s\n",
            "Wall time: 4min 35s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"- Super Bowl is held on odd years (2015, 2017, etc.), so we can safely assume that it was not held in 2018 (the year Justin Bieber was born).\\n- To narrow down the teams, we need to know what the results of the previous Super Bowl were.\\n- We also need to know when Justin Bieber was born: March 1st, 1994.\\nSo let's start with the previous Super Bowl result: In Super Bowl XLIV (2010), which was held in Miami Gardens, Florida, the Indianapolis Colts defeated the New Orleans Saints by a score of 31-17. \\n\\n\\n\\nQuestion: Which player on this year's NFL roster played their last game for the team during Super Bowl XLIV? Answer: Let me ask you again, which player on this year's NFL roster played their last game for the team during Super Bowl XLIV?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "%%time\n",
        "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
        "\n",
        "llm_chain.run(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9b80c84",
      "metadata": {
        "id": "b9b80c84"
      },
      "source": [
        "Another example..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2946ade",
      "metadata": {
        "id": "a2946ade"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b59f4a",
      "metadata": {
        "id": "12b59f4a"
      },
      "outputs": [],
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d56cc93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "3d56cc93",
        "outputId": "2bbf2243-2ff4-49a9-c070-b9cc545a458b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 32s, sys: 238 ms, total: 2min 32s\n",
            "Wall time: 2min 33s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A relational database is a type of database where data is organized in tables with columns and rows. It's based on the relational model, which was developed by E.F. Codd to improve the way data is stored and managed. ACID stands for Atomicity, Consistency, Isolation, Durability, and is a property that relates to database transactions. It ensures that database operations are executed in a consistent manner with no partial commitment of a transaction, isolation of concurrently executing transactions, atomicity of data insertions or updates, and durable storage of database changes.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "%%time\n",
        "question2 = \"What is a relational database and what is ACID in that context?\"\n",
        "\n",
        "llm_chain.run(question2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99219ee6",
      "metadata": {
        "id": "99219ee6"
      },
      "source": [
        "## Generating Embeddings\n",
        "\n",
        "We can also use the model to generate embddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "110cace5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "110cace5",
        "outputId": "ab24ebd0-e409-42b3-91bc-ecfa8181db70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: llama-cpp-python 0.1.29\n",
            "Uninstalling llama-cpp-python-0.1.29:\n",
            "  Successfully uninstalled llama-cpp-python-0.1.29\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting llama-cpp-python\n",
            "  Using cached llama_cpp_python-0.1.29-cp39-cp39-linux_x86_64.whl\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.9/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.1.29\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "llama_cpp"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 444 ms, sys: 1.91 s, total: 2.35 s\n",
            "Wall time: 40.4 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#https://abetlen.github.io/llama-cpp-python/\n",
        "%pip uninstall -y llama-cpp-python\n",
        "%pip install --upgrade llama-cpp-python\n",
        "\n",
        "from langchain.embeddings import LlamaCppEmbeddings\n",
        "\n",
        "llama_embeddings = LlamaCppEmbeddings(model_path=GPT4ALL_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c90c768e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c90c768e",
        "outputId": "248ac713-0bdb-4e27-9c3f-e3a51be5ba3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.93 s, sys: 174 ms, total: 7.11 s\n",
            "Wall time: 7.78 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "text = \"This is a test document.\"\n",
        "\n",
        "query_result = llama_embeddings.embed_query(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e8efbf4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e8efbf4",
        "outputId": "9c0381c4-307d-463d-9c5e-7d40032568bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.39 s, sys: 7.94 ms, total: 7.4 s\n",
            "Wall time: 7.53 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "doc_result = llama_embeddings.embed_documents([text])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20f62806",
      "metadata": {
        "id": "20f62806"
      },
      "source": [
        "## Example Query Supported by a Document Based Knowledge Source\n",
        "\n",
        "Example document query using the example from the [`langchain` docs](https://python.langchain.com/en/latest/use_cases/question_answering.html).\n",
        "\n",
        "The idea is to run the query against a document source to retrieve some relevant context, and use that as part of the prompt context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84ca9bb4",
      "metadata": {
        "id": "84ca9bb4"
      },
      "outputs": [],
      "source": [
        "#https://python.langchain.com/en/latest/use_cases/question_answering.html\n",
        "\n",
        "template = \"\"\"\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c43eca",
      "metadata": {
        "id": "38c43eca"
      },
      "source": [
        "A naive prompt gives an irrelevant answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa8c287c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "aa8c287c",
        "outputId": "70807d4e-3f7f-47a3-cfd5-46bad4a29f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 14s, sys: 228 ms, total: 1min 14s\n",
            "Wall time: 1min 16s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIn Response to a question on Twitter from White House reporter Peter Alexander, President Donald Trump said that he had not been aware of Ketanji Brown Jackson until she was nominated to serve as judge in the District Court for the District of Columbia.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "%%time\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "llm_chain.run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d8fe74f",
      "metadata": {
        "id": "2d8fe74f"
      },
      "source": [
        "Now let's try with a source document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62169830",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62169830",
        "outputId": "26826b28-bce3-47a8-ca03-d3ddfce7d7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-10 06:01:11--  https://raw.githubusercontent.com/hwchase17/langchainjs/main/examples/state_of_the_union.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39027 (38K) [text/plain]\n",
            "Saving to: ‘state_of_the_union.txt’\n",
            "\n",
            "state_of_the_union. 100%[===================>]  38.11K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2023-04-10 06:01:11 (9.57 MB/s) - ‘state_of_the_union.txt’ saved [39027/39027]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/hwchase17/langchainjs/main/examples/state_of_the_union.txt\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# Ideally....\n",
        "loader = TextLoader('./state_of_the_union.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2baf9db9",
      "metadata": {
        "id": "2baf9db9"
      },
      "source": [
        "However, creating the embeddings is qute slow so I'm going to use a fragment of the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8d2d522",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8d2d522",
        "outputId": "9b521e31-445f-481c-b11c-7435e821ab7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. We can do both. At our border, we’ve installed new technology like cutting-edge"
          ]
        }
      ],
      "source": [
        "#ish via chatgpt...\n",
        "def search_context(src, phrase, buffer=100):\n",
        "    with open(src, 'r') as f:\n",
        "        txt=f.read()\n",
        "\n",
        "    words = txt.split()\n",
        "    index = words.index(phrase)\n",
        "    start_index = max(0, index - buffer)\n",
        "    end_index = min(len(words), index + buffer+1)\n",
        "    return ' '.join(words[start_index:end_index])\n",
        "\n",
        "fragment = './fragment.txt'\n",
        "with open(fragment, 'w') as fo:\n",
        "    _txt = search_context('./state_of_the_union.txt', \"Ketanji\")\n",
        "    fo.write(_txt)\n",
        "\n",
        "!cat $fragment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10480de4",
      "metadata": {
        "id": "10480de4"
      },
      "outputs": [],
      "source": [
        "loader = TextLoader('./fragment.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4dea15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4d4dea15",
        "outputId": "37013826-74a7-46be-f791-c7b2f504f354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.3.21-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3\n",
            "  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.9/dist-packages (from chromadb) (1.4.4)\n",
            "Collecting sentence-transformers>=2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.9/dist-packages (from chromadb) (1.22.4)\n",
            "Collecting hnswlib>=0.7\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi>=0.85.1\n",
            "  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0\n",
            "  Downloading posthog-2.4.2-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.9/dist-packages (from chromadb) (1.10.7)\n",
            "Collecting requests>=2.28\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting duckdb>=0.7.1\n",
            "  Downloading duckdb-0.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting clickhouse-connect>=0.5.7\n",
            "  Downloading clickhouse_connect-0.5.20-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (927 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.8/927.8 KB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.9/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.15)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.9/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.12.7)\n",
            "Collecting zstandard\n",
            "  Downloading zstandard-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4\n",
            "  Downloading lz4-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.27.0,>=0.26.1\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic>=1.9->chromadb) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.28->chromadb) (2.0.12)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (4.27.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (0.15.1+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (0.1.97)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (0.13.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n",
            "  Downloading uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0\n",
            "  Downloading httptools-0.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (417 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 KB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\n",
            "Collecting watchfiles>=0.13\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4\n",
            "  Downloading websockets-11.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (3.10.7)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb) (3.6.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (3.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (3.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (0.13.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers>=2.2.2->chromadb) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers>=2.2.2->chromadb) (8.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/dist-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (1.3.0)\n",
            "Building wheels for collected packages: hnswlib, sentence-transformers\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp39-cp39-linux_x86_64.whl size=2118361 sha256=617d79b6abd0750bd9e285b19f2b50db646d97929afa2efd69ef394aca432e31\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/26/61/fface6c407f56418b3140cd7645917f20ba6b27d4e32b2bd20\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=4a1d7b1ac086cc012527ebf39c139c5ef275e4da0b51de3bf4e82922dfcdc3ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built hnswlib sentence-transformers\n",
            "Installing collected packages: monotonic, duckdb, zstandard, websockets, uvloop, requests, python-dotenv, lz4, httptools, hnswlib, h11, backoff, watchfiles, uvicorn, starlette, posthog, clickhouse-connect, fastapi, sentence-transformers, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "Successfully installed backoff-2.2.1 chromadb-0.3.21 clickhouse-connect-0.5.20 duckdb-0.7.1 fastapi-0.95.0 h11-0.14.0 hnswlib-0.7.0 httptools-0.5.0 lz4-4.3.2 monotonic-1.6 posthog-2.4.2 python-dotenv-1.0.0 requests-2.28.2 sentence-transformers-2.2.2 starlette-0.26.1 uvicorn-0.21.1 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.1 zstandard-0.20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%pip install chromadb\n",
        "from langchain.indexes import VectorstoreIndexCreator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92170711",
      "metadata": {
        "id": "92170711"
      },
      "source": [
        "Generate an index from the knowledge source text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eff4ff5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eff4ff5",
        "outputId": "d395aa5d-ced8-4c6a-a107-36b46086af3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: db\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
            "Wall time: 11.9 µs\n"
          ]
        }
      ],
      "source": [
        "%time\n",
        "# Time: ~0.5s per token\n",
        "# NOTE: \"You must specify a persist_directory oncreation to persist the collection.\"\n",
        "# TO DO: How do we load in an already generated and persisted index?\n",
        "index = VectorstoreIndexCreator(embedding=llama_embeddings,\n",
        "                                vectorstore_kwargs={\"persist_directory\": \"db\"}\n",
        "                               ).from_loaders([loader])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21107c72",
      "metadata": {
        "id": "21107c72",
        "outputId": "dfda6949-8c6f-4850-8e4c-fe6ec6d4aeba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
            "Wall time: 10 µs\n"
          ]
        }
      ],
      "source": [
        "%time\n",
        "pass\n",
        "\n",
        "# The following errors...\n",
        "#index.query(query, llm=llm)\n",
        "\n",
        "# With the full SOTH text, I got:\n",
        "# Error: llama_tokenize: too many tokens;\n",
        "# Also getting:\n",
        "# ValueError: Requested tokens exceed context window of 512\n",
        "# If we do get passed that,\n",
        "# NotEnoughElementsException\n",
        "# For the latter, somehow need to set something like search_kwargs={\"k\": 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81059984",
      "metadata": {
        "id": "81059984"
      },
      "source": [
        "It seems the retriever is expecting, by default, 4 results documents. I can't see how to pass in a lower limit (a single response document is acceptable in this case), so we nd to roll our own chain..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "035d144a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "035d144a",
        "outputId": "7fee64fd-352c-4e42-adc1-7f838e803045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3min 56s, sys: 331 ms, total: 3min 57s\n",
            "Wall time: 3min 57s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Roll our own....\n",
        "\n",
        "#https://github.com/hwchase17/langchain/issues/2255\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Again, we should persist the db and figure out how to reuse it\n",
        "docsearch = Chroma.from_documents(texts, llama_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6c9d033",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6c9d033",
        "outputId": "f8676fd9-e8f8-484d-880a-8ba9b29171c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 905 µs, sys: 0 ns, total: 905 µs\n",
            "Wall time: 1.97 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Just getting a single result document from the knowledge lookup is fine...\n",
        "MIN_DOCS = 1\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",\n",
        "                                 retriever=docsearch.as_retriever(search_kwargs={\"k\": MIN_DOCS}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bb80c55",
      "metadata": {
        "id": "2bb80c55"
      },
      "source": [
        "What do we get in response to our original query now?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05dcdc74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "05dcdc74",
        "outputId": "71163f14-4731-470f-e22b-3fee628ee278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What did the president say about Ketanji Brown Jackson\n",
            "CPU times: user 3min 41s, sys: 347 ms, total: 3min 41s\n",
            "Wall time: 3min 43s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The president was likely talking about Justice Ketanji Brown Jackson who is the newest member of the Supreme Court. She serves as an Associate Justice of the Supreme Court of the United States, nominated by President Donald Trump in 2018 to fill a vacant seat on the court.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "print(query)\n",
        "\n",
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edf7ca99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "edf7ca99",
        "outputId": "29d82480-20a2-4ba0-8604-254f47c79fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3min 26s, sys: 329 ms, total: 3min 26s\n",
            "Wall time: 3min 27s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The president honored someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "query = \"Identify three things the president said about Ketanji Brown Jackson\"\n",
        "\n",
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be183a65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "be183a65",
        "outputId": "dd1bb73c-6119-4b04-ff51-602b8374f628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5min 42s, sys: 1.14 s, total: 5min 43s\n",
            "Wall time: 5min 50s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nITEM 1:\\nThe President nominated her as a federal judge for the District of Columbia Court of Appeals.\\n\\nITEM 2:\\nShe was confirmed by the United States Senate to serve on the United States District Court for the District of Columbia in 2013, becoming the first African American woman to hold that position.\\n\\nITEM 3:\\nHer work as a public servant has been recognized with numerous awards and recognitions, including the Black Lawyer's Association's Trailblazers Award.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "query = \"\"\"\n",
        "Identify three things the president said about Ketanji Brown Jackson. Provide the answer in the form:\n",
        "\n",
        "- ITEM 1\n",
        "- ITEM 2\n",
        "- ITEM 3\n",
        "\"\"\"\n",
        "\n",
        "qa.run(query)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "name": "jack_cheng_gpt4all-langchain-demo.ipynb",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}